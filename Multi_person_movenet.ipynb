{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ad13bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c04175cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99760a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6775f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load('https://tfhub.dev/google/movenet/multipose/lightning/1')\n",
    "movenet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1030ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to loop through each person detected and render\n",
    "def loop_through_people(frame, keypoints_with_scores, edges, confidence_threshold):\n",
    "    for person in keypoints_with_scores:\n",
    "        draw_connections(frame, person, edges, confidence_threshold)\n",
    "        draw_keypoints(frame, person, confidence_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb8a72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 6, (0,255,0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac175d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738a2a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3d1ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_to_centre(b_box,centre, h,w):\n",
    "    # data in b_box: x_min, Y_min, x_max, y_max, confidence/score\n",
    "    conf = np.array(b_box[:,4])\n",
    "    n_people = (conf>0.2).sum()\n",
    "    if (n_people<=1):\n",
    "        res = np.argmax(conf) #most prominent\n",
    "        centroid = [((b_box[res][0]+b_box[res][2])/2)*w,((b_box[res][1]+b_box[res][3])/2)*h]\n",
    "    else:\n",
    "        #to find closest to center the euclidean dist. of the centroid of the bounding must be closest to centre pixel\n",
    "        res = -1\n",
    "        dis = 1000000\n",
    "        for i in range (6):\n",
    "            if (b_box[i][4]<0.2):\n",
    "                continue\n",
    "            c = [((b_box[i][0]+b_box[i][2])/2)*w,((b_box[i][1]+b_box[i][3])/2)*h]\n",
    "            new = math.dist(centre,c)\n",
    "            if (new<dis):\n",
    "                dis = new\n",
    "                res = i\n",
    "                centroid = c\n",
    "    return res,centroid\n",
    "#Major assumption - subject starts off closest to centre of the frame, irrespective of who is the most prominent in the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f015db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(y1,x1,y2,x2,y3,x3):\n",
    "\n",
    "    # Calculate the angle between the three points\n",
    "    angle = math.degrees(math.atan2(y3 - y2, x3 - x2) - math.atan2(y1 - y2, x1 - x2))\n",
    "    \n",
    "    # Check if the angle is less than zero.\n",
    "    if angle < 0:\n",
    "        \n",
    "        # Add 360 to the found angle.\n",
    "        angle += 360\n",
    "   \n",
    "    # Return the calculated angle.\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5994570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pose(lka,rka):\n",
    "    label = 'none'\n",
    "    if ((lka<120 or lka > 240) or (rka<120 or rka>240)):\n",
    "        label = 'sitting'\n",
    "    else:\n",
    "        label = 'standing'\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30a1b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('D:\\LifeSpark Technology\\Gait Data\\SP\\DJI_0113.mp4')\n",
    "final = []\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "# #fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "centroid = [w/2,h/2]\n",
    "#video_writer = cv2.VideoWriter('KS_113_movenet.avi', cv2.VideoWriter_fourcc('P','I','M','1'), fps, (width, height), isColor=True) \n",
    "f=1\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Resize image\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 288,512)\n",
    "    input_img = tf.cast(img, dtype=tf.int32)\n",
    "    \n",
    "    # Detection section\n",
    "    results = movenet(input_img)\n",
    "    b_box = results['output_0'].numpy()[:,:,51:].reshape((6,5))\n",
    "    keypoints_with_scores = results['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "    \n",
    "    #to find subject among others, initial frame: person nearest centre\n",
    "    data = []\n",
    "    key,centroid = find_closest_to_centre(b_box,centroid,h,w)   #centroid keeps changing, so same person is being tracked\n",
    "    lka = calculate_angle(keypoints_with_scores[key][11][0]*h,keypoints_with_scores[key][11][1]*w,keypoints_with_scores[key][13][0]*h,keypoints_with_scores[key][13][1]*w,keypoints_with_scores[key][15][0]*h,keypoints_with_scores[key][15][1]*w)\n",
    "    rka = calculate_angle(keypoints_with_scores[key][12][0]*h,keypoints_with_scores[key][12][1]*w,keypoints_with_scores[key][14][0]*h,keypoints_with_scores[key][14][1]*w,keypoints_with_scores[key][16][0]*h,keypoints_with_scores[key][16][1]*w)\n",
    "    data.append(lka)\n",
    "    data.append(rka)\n",
    "    label = classify_pose(lka,rka)\n",
    "    data.append(label)\n",
    "    time = int(f/fps)\n",
    "    f+=1\n",
    "    data.append(time)\n",
    "    final.append(data)\n",
    "    # Render keypoints \n",
    "    \n",
    "    keypoints_with_scores = keypoints_with_scores[key].reshape((1,17,3))  #comment this line to view all keypoints // uncomment to view only extracted keypoint\n",
    "    \n",
    "    loop_through_people(frame, keypoints_with_scores, EDGES, 0.3)\n",
    "    #video_writer.write(frame)\n",
    "    frame = cv2.resize(frame,(960,540))\n",
    "    cv2.putText(frame, label, (10, 30),cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2)\n",
    "    cv2.imshow('Movenet Multipose', frame)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "#video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f67486e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d02fc077",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40dc1760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b95c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_box = np.array([[0.22368601, 0.42062107, 0.7792237 , 0.51973635, 0.6374854 ],\n",
    "       [0.4017529 , 0.6249735 , 0.9211679 , 0.77323395, 0.29471096],\n",
    "       [0.49285173, 0.7180296 , 0.8518226 , 0.885344  , 0.27043197],\n",
    "       [0.4798509 , 0.8897557 , 0.9787918 , 0.9954388 , 0.20403856],\n",
    "       [0.9190839 , 0.02295218, 0.95632005, 0.03121129, 0.        ],\n",
    "       [0.8638335 , 0.9166504 , 1.        , 0.9974245 , 0.00485678]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22d66174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6374854 , 0.29471096, 0.27043197, 0.20403856, 0.        ,\n",
       "       0.00485678])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = np.array(b_box[:,4])\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36849f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.38623562, 0.65775865, 0.32200402],\n",
       "        [0.37765947, 0.661859  , 0.2423978 ],\n",
       "        [0.37639356, 0.65946805, 0.37453616],\n",
       "        [0.38749722, 0.6868141 , 0.40928015],\n",
       "        [0.38158128, 0.67465687, 0.52859896],\n",
       "        [0.44342932, 0.7245306 , 0.6959927 ],\n",
       "        [0.42943874, 0.656787  , 0.559538  ],\n",
       "        [0.5649513 , 0.7449355 , 0.38657108],\n",
       "        [0.50232345, 0.633692  , 0.46407226],\n",
       "        [0.5816127 , 0.73135847, 0.12678204],\n",
       "        [0.47134003, 0.62293947, 0.14948112],\n",
       "        [0.6165895 , 0.7020366 , 0.37259623],\n",
       "        [0.59084797, 0.6594647 , 0.42283297],\n",
       "        [0.6438366 , 0.6911451 , 0.19768706],\n",
       "        [0.590214  , 0.63175434, 0.24949914],\n",
       "        [0.7486596 , 0.6781819 , 0.2906392 ],\n",
       "        [0.75537753, 0.67900693, 0.34597418]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9957ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['nose', 'left eye', 'right eye', 'left ear', 'right ear', 'left shoulder', 'right shoulder', 'left elbow', 'right elbow', 'left wrist', 'right wrist', 'left hip', 'right hip', 'left knee', 'right knee', 'left ankle', 'right ankle']\n",
    "coord = ['x','y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "357accb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[163.40251289918072, 97.29701710234302, 'sitting', 0],\n",
       " [163.26139883235768, 98.4452519689806, 'sitting', 0],\n",
       " [163.89144538332877, 97.61610387007518, 'sitting', 0],\n",
       " [164.57529837647576, 97.16110762124825, 'sitting', 0],\n",
       " [163.9930370125961, 97.13430666949102, 'sitting', 0],\n",
       " [162.56940852844298, 96.89920731865868, 'sitting', 0],\n",
       " [163.08785451187939, 97.30316292731517, 'sitting', 0],\n",
       " [163.81564379344562, 97.89857866768493, 'sitting', 0],\n",
       " [162.38240451541293, 96.73159217816773, 'sitting', 0],\n",
       " [162.25290615498386, 96.70959642426337, 'sitting', 0],\n",
       " [161.69480782691568, 95.96796095281577, 'sitting', 0],\n",
       " [161.6444300775477, 96.00969877338322, 'sitting', 0],\n",
       " [162.23707800508458, 96.22729779108423, 'sitting', 0],\n",
       " [161.74267066381395, 96.08580502969444, 'sitting', 0],\n",
       " [161.88423236984005, 95.95315193564196, 'sitting', 0],\n",
       " [162.18734167320625, 96.10033058467766, 'sitting', 0],\n",
       " [162.49653225736162, 96.07372079499942, 'sitting', 0],\n",
       " [162.6012674051601, 95.95944758394796, 'sitting', 0],\n",
       " [162.85506316313106, 96.03746195973423, 'sitting', 0],\n",
       " [163.4951864614544, 96.04161493443814, 'sitting', 0],\n",
       " [161.7027801280016, 95.30049030701164, 'sitting', 0],\n",
       " [161.98287258034776, 95.54740222934376, 'sitting', 0],\n",
       " [161.55638252988277, 95.26045034347257, 'sitting', 0],\n",
       " [161.4231742091884, 94.87296990367095, 'sitting', 0],\n",
       " [161.23261564412627, 95.15744116501985, 'sitting', 0],\n",
       " [161.93621436575992, 95.22809640299826, 'sitting', 0],\n",
       " [161.61302196279652, 95.0019497276821, 'sitting', 0],\n",
       " [161.7487408157368, 95.08465576079216, 'sitting', 0],\n",
       " [161.95528770713975, 95.17899966824, 'sitting', 1],\n",
       " [161.6814043546397, 95.7087498588115, 'sitting', 1],\n",
       " [162.44210351150815, 95.31996463342315, 'sitting', 1],\n",
       " [162.319899721491, 95.2998449521828, 'sitting', 1],\n",
       " [162.27237758528625, 95.92718732707584, 'sitting', 1],\n",
       " [161.8064113383649, 95.7764257951786, 'sitting', 1]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f42fd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[161.8064113383649, 95.7764257951786, 'sitting', 1]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "918d3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['left_knee_angle','right_knee_angle','label','timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac72549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"filename\",\"w+\") as my_csv:\n",
    "    w = csv.writer(my_csv,delimiter=',', lineterminator = '\\n')\n",
    "    w.writerow(headers)\n",
    "    w.writerows(final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
