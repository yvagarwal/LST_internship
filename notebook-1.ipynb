{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8816e4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.4.2-cp39-cp39-win_amd64.whl (10.5 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\yash vardhan agarwal\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\yash vardhan agarwal\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yash vardhan agarwal\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.4.2 pytz-2022.1\n",
      "Collecting ruptures\n",
      "  Using cached ruptures-1.1.6-cp39-cp39-win_amd64.whl (378 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\yash vardhan agarwal\\appdata\\roaming\\python\\python39\\site-packages (from ruptures) (1.19.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\yash vardhan agarwal\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from ruptures) (1.7.3)\n",
      "Installing collected packages: ruptures\n",
      "Successfully installed ruptures-1.1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install ruptures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "754d4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import ruptures as rpt\n",
    "import csv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ef13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')   #for gpu usage\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c07fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load('https://tfhub.dev/google/movenet/multipose/lightning/1')\n",
    "movenet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a888c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to loop through each person detected and render\n",
    "def loop_through_people(frame, keypoints_with_scores, edges, confidence_threshold):\n",
    "    for person in keypoints_with_scores:\n",
    "        draw_connections(frame, person, edges, confidence_threshold)\n",
    "        draw_keypoints(frame, person, confidence_threshold)\n",
    "\n",
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 6, (0,255,0), -1)\n",
    "\n",
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3fd58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_to_centre(b_box,centre, h,w):\n",
    "    # data in b_box: x_min, Y_min, x_max, y_max, confidence/score\n",
    "    conf = np.array(b_box[:,4])\n",
    "    n_people = (conf>0.2).sum()\n",
    "    if (n_people<=1):\n",
    "        res = np.argmax(conf) #most prominent\n",
    "        centroid = [((b_box[res][0]+b_box[res][2])/2)*w,((b_box[res][1]+b_box[res][3])/2)*h]\n",
    "    else:\n",
    "        #to find closest to center the euclidean dist. of the centroid of the bounding must be closest to centre pixel\n",
    "        res = -1\n",
    "        dis = 1000000\n",
    "        for i in range (6):\n",
    "            if (b_box[i][4]<0.2):\n",
    "                continue\n",
    "            c = [((b_box[i][0]+b_box[i][2])/2)*w,((b_box[i][1]+b_box[i][3])/2)*h]\n",
    "            new = math.dist(centre,c)\n",
    "            if (new<dis):\n",
    "                dis = new\n",
    "                res = i\n",
    "                centroid = c\n",
    "    return res, centroid\n",
    "#Major assumption - subject starts off closest to centre of the frame, irrespective of who is the most prominent in the frame\n",
    "\n",
    "def calculate_angle(y1,x1,y2,x2,y3,x3):\n",
    "\n",
    "    # Calculate the angle between the three points\n",
    "    angle = math.degrees(math.atan2(y3 - y2, x3 - x2) - math.atan2(y1 - y2, x1 - x2))\n",
    "    \n",
    "    # Check if the angle is less than zero.\n",
    "    if angle < 0:\n",
    "        \n",
    "        # Add 360 to the found angle.\n",
    "        angle += 360\n",
    "   \n",
    "    # Return the calculated angle.\n",
    "    return angle\n",
    "\n",
    "def classify_pose(lka,rka):\n",
    "    label = 'none'\n",
    "    if ((lka<120 or lka > 240) or (rka<120 or rka>240)):\n",
    "        label = 'sitting'\n",
    "    else:\n",
    "        label = 'standing'\n",
    "    \n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8687e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'D:\\LifeSpark Technology\\videos'  \n",
    "files = glob.glob(os.path.join(dir, '*.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "644d4328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\LifeSpark Technology\\\\check\\\\DJI_0230_conv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a0117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    cap = cv2.VideoCapture(file)\n",
    "    final = []\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    # #fourcc = cv2.VideoWriter_fourcc(*'XVID')  #for saving video\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    centroid = [w/2,h/2]\n",
    "    video_writer = cv2.VideoWriter(file[:-4]+'.avi', cv2.VideoWriter_fourcc('P','I','M','1'), fps, (w, h), isColor=True) \n",
    "    f=1\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Resize image\n",
    "        img = frame.copy()\n",
    "        img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 192,288)  #movenet requires multiples of 32\n",
    "        input_img = tf.cast(img, dtype=tf.int32)\n",
    "\n",
    "        # Detection section\n",
    "        results = movenet(input_img)\n",
    "        b_box = results['output_0'].numpy()[:,:,51:].reshape((6,5))\n",
    "        keypoints_with_scores = results['output_0'].numpy()[:,:,:51].reshape((6,17,3))\n",
    "\n",
    "        #to find subject among others, considering in initial frame, subject is the person nearest centre\n",
    "        data = []\n",
    "        \n",
    "        \n",
    "        key, centroid = find_closest_to_centre(b_box,centroid,h,w)   \n",
    "        #centroid keeps changing, so that same person is being tracked, Some videos may give better results when centroid is fixed\n",
    "        \n",
    "        \n",
    "        lka = calculate_angle(keypoints_with_scores[key][11][0]*h,keypoints_with_scores[key][11][1]*w,keypoints_with_scores[key][13][0]*h,keypoints_with_scores[key][13][1]*w,keypoints_with_scores[key][15][0]*h,keypoints_with_scores[key][15][1]*w)\n",
    "        rka = calculate_angle(keypoints_with_scores[key][12][0]*h,keypoints_with_scores[key][12][1]*w,keypoints_with_scores[key][14][0]*h,keypoints_with_scores[key][14][1]*w,keypoints_with_scores[key][16][0]*h,keypoints_with_scores[key][16][1]*w)\n",
    "        data.append(lka)\n",
    "        data.append(rka)\n",
    "        label = classify_pose(lka,rka)\n",
    "        data.append(label)\n",
    "        time = int(f/fps)\n",
    "        f+=1\n",
    "        data.append(time)\n",
    "        final.append(data)\n",
    "        # Render keypoints \n",
    "\n",
    "        #keypoints_with_scores = keypoints_with_scores[key].reshape((1,17,3))  #comment this line to view all keypoints // uncomment to view only extracted keypoint\n",
    "\n",
    "        loop_through_people(frame, keypoints_with_scores, EDGES, 0.3)\n",
    "        video_writer.write(frame)\n",
    "        #frame = cv2.resize(frame,(960,540))\n",
    "        cv2.putText(frame, label, (10, 30),cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 2)\n",
    "        cv2.imshow('Movenet Multipose', frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    video_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    headers = ['left_knee_angle','right_knee_angle','label','timestamp']\n",
    "    with open(file[:-4]+'.csv',\"w+\") as my_csv:     #saving data at similar path \n",
    "        w = csv.writer(my_csv,delimiter=',', lineterminator = '\\n')\n",
    "        w.writerow(headers)\n",
    "        w.writerows(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8525e48d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
